{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6083f220",
   "metadata": {},
   "source": [
    "**Caleb and Grady**\n",
    "\n",
    "Spring 2023\n",
    "\n",
    "CS 443: Bio-inspired Machine Learning\n",
    "\n",
    "#### Week 3: Train decoders to classify MNIST from Hebbian network activations\n",
    "\n",
    "# Project 1: Hebbian Learning\n",
    "\n",
    "You will use your linear and nonlinear decoders to predict the digit classification accuracy on MNIST based on the learned Hebbian representation (i.e. weights) that you saved last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81130992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "\n",
    "from mnist import get_mnist\n",
    "from hebb_net import HebbNet\n",
    "\n",
    "# plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.show()\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f46184",
   "metadata": {},
   "source": [
    "## Task 5: Train and test decoders on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b6a636",
   "metadata": {},
   "source": [
    "### 5a. Preparing decoder inputs\n",
    "\n",
    "In the cell below:\n",
    "- load the raw MNIST train/test/validation samples. Reserve 2000 training samples for validation.\n",
    "- process them with your the Hebbian network (i.e. compute their corresponding netIn values) to get the input for your decoders.\n",
    "- Cast each of the train/test/validation values to the `tf.float32` datatype.\n",
    "\n",
    "**Tips:**\n",
    "- Your Hebbian network constructor has a keyword argument that you can use to load wts from a previously trained network. You should not need to retrain your Hebbian network!\n",
    "- When creating your Hebbian network object, remember to build it with the same hyperparameters as you did last week (e.g. number of neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, x_val, y_val = get_mnist(2000)\n",
    "mnist_data_list = [x_train, x_test, x_val]\n",
    "\n",
    "net = HebbNet(784, 500, load_wts=True)\n",
    "\n",
    "mnist_net_in_list = [net.net_in(data_set) for data_set in mnist_data_list]\n",
    "mnist_net_in_tensors = [tf.convert_to_tensor(net_in, dtype=tf.float32) for net_in in mnist_net_in_list]\n",
    "print(mnist_net_in_tensors[0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de17596a",
   "metadata": {},
   "source": [
    "### 5b. Linear decoder\n",
    "\n",
    "Train your softmax classifier on the Hebbian network `net_in` activations obtained from processing the MNIST training set. The hyperparameters are up to you, but you should generally make principled choices (see question below).\n",
    "\n",
    "#### Guidelines\n",
    "\n",
    "- Training should be fairly quick (no more than a few minutes).\n",
    "- Make use of the validation set to monitor progress while training.\n",
    "- **Remember:** you are **NOT** training the softmax classifier **on MNIST** — you are training it on the `net_in` values produced by the Hebbian network that you trained above!\n",
    "- Checking the validation accuracy too frequently will slow down training, but if you check too infrequently you won't know what's going on.\n",
    "\n",
    "\n",
    "#### Results\n",
    "\n",
    "Here are the items that you should report in the cell(s) below:\n",
    "\n",
    "- Accuracy of the softmax classifier on the test set.\n",
    "- Create a well-labeled plot showing the training and validation loss over epochs (*note: because you likely did not check validation loss on each epoch, but \"x\" epoch values for each loss will likely be different. Account for this when generating \"x\" values used in the plot.*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5020bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_decoder import SoftmaxDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c41ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279db83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6969aeb",
   "metadata": {},
   "source": [
    "### 5c. Nonlinear decoder\n",
    "\n",
    "Repeat what you did for the softmax classifier with the nonlinear classifier.\n",
    "\n",
    "**Note there is one additional step:** Once you get the Hebbian network `net_in` values for the train/validation/test sets, the nonlinear decoding network proposed by Krotov & Hopfield (2019) assumes that the Hebbian network `net_in` values ($h_{ij}$) that serve as the input to the decoder are transformed by the following activation function:\n",
    "\n",
    "$$x_{ij} = max(h_{ij}, 0)^n$$\n",
    "\n",
    "where $h_{ij}$ are the Hebbian network `net_in` values. In other words, apply ReLU to the `net_in` values then raise the result to the power `n`. By default, we assume that the hyperparameter $n=5$.\n",
    "\n",
    "**This additional ReLU step needs to be performed on the `net_in` values representing each of the decoder train/validation/test sets!!**\n",
    "\n",
    "\n",
    "#### Guidelines\n",
    "\n",
    "- Training should be fairly quick, but it might take a few minutes longer than the softmax network due to the added complexity.\n",
    "- The max epochs will need to be set much higher than for the softmax decoder (at least several thousand).\n",
    "- You likely will need different hyperparameter values here than the softmax network.\n",
    "- Try using the default $\\beta = 0.005$ and $m = 6$ (loss function exponent) hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_decoder import NonlinearDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68dda4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nonlinear(x, n=5):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ead1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a8f90",
   "metadata": {},
   "source": [
    "### 5d. Questions\n",
    "\n",
    "**Question 9:** Reflect on the relative performance of the linear and nonlinear decoders. In at least one paragraph, compare and contrast pros/cons. \n",
    "\n",
    "**Question 10:** Explain your reasoning for picking the hyperparameters that you did for either decoder.\n",
    "\n",
    "**Question 11:** Which decoder requires more epochs to train until the validation loss/accuracy plateaus? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8497652f",
   "metadata": {},
   "source": [
    "**Answer 9:** \n",
    "\n",
    "**Answer 10:** \n",
    "\n",
    "**Answer 11:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d51b1",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0665b5",
   "metadata": {},
   "source": [
    "### 0. Compare encoder-decoder model to end-to-end training\n",
    "\n",
    "Compare how accurately the linear and nonlinear decoders learn from raw MNIST samples compared to from the Hebbian network activations (*I would suggest keeping hyperparameters constant for a fair comparison*). There is a lot to explore here! Here are a few questions to examine:\n",
    "- How rapidly the decoders networks learn their inputs (e.g. number of training epochs needed to achieve \"good\" accuracy on the validation set)?\n",
    "- What is the best test accuracy achieved by the decoders networks with the same hyperparameters?\n",
    "- How does the decoder training times compare with the Hebbian inputs vs raw MNIST samples to achieve some level of accuracy?\n",
    "- Remember that you control the dimension of the \"embedding\" performed by the Hebbian network (i.e. number of neurons in the net). How does the accuracy and/or training time of the decoders trade off with the Hebbian network embedding size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c8757",
   "metadata": {},
   "source": [
    "### 1. Use your CS 343 Softmax network as the linear decoder\n",
    "\n",
    "This will require a few updates to support the Adam optimizer (that you implemented in the CS 343 CNN project) and validation sets.\n",
    "\n",
    "Copy `softmax_layer.py` from your CS343 MLP project to your working directory. Also copy `optimizer.py` from your CS343 CNN project.\n",
    "\n",
    "Make the following changes to `fit()` in `softmax_layer.py`:\n",
    "1. Switch your optimizer from SGD to Adam. This will involve creating two `Adam` objects: one for the weights, one for the bias. Also, be sure to set the Adam learning rate based on the value passed into `fit()`.\n",
    "2. Add support in `fit()` for a validation set by adding the keyword arguments: `x_val=None, y_val=None`. If `verbose > 0` print out the accuracy and loss over the entire validation set. \n",
    "3. If `verbose > 0` convert your print outs to happen in terms of epochs rather than iterations (e.g. every epoch, not every 100 iterations). Add a keyword argument `val_freq=50` to specify how often (in epochs) to check and print out the validation accuracy and loss. Be sure to always print out the validation accuracy and loss on the first and last epoch regardless of the `val_freq` value.\n",
    "4. Have `fit()` return both the train and validation loss as Python lists or ndarrays. In cases when you do not pass in a validation set, the returned validation loss list may be `None` and that's ok.\n",
    "\n",
    "The network should train similarily to your Tensorflow version. Compare/analyze runtime performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18af83",
   "metadata": {},
   "source": [
    "### 2. Encode an image dataset of your choice with the Hebbian network\n",
    "\n",
    "For example, Fashion MNIST, STL-10 or CIFAR-10. If your images contain color, I suggest either converting to grayscale or flattening the color channels when constructing your feature vectors (e.g. `(32, 32, 3)` color image made into a `(3072,)` vector). Note that color images will clearly take much longer to train.\n",
    "\n",
    "Some areas to explore:\n",
    "- Visualize the weights. Analyze how hyperparameters affect the structure.\n",
    "- Compare decoding accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131579c",
   "metadata": {},
   "source": [
    "### 3. Learning rate decay\n",
    "\n",
    "Krotov & Hopfield (2019) decayed the learning rate according to a epoch-based schedule for both the Hebbian network and the decoder (see their Appendix B for details). Implement this or your own variant (for either encoder and/or decoder network) and explore whether it improves decoding performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a9301a",
   "metadata": {},
   "source": [
    "### 4. Hyperparameter tuning\n",
    "\n",
    "Use a grid or random search for encoder and/decoder networks to optimize performance.\n",
    "\n",
    "How does the number of \"runner-up\" neurons in the Hebbian network ($K$) influence the learned weights (visually) or the accuracy with which either decoding network decodes the correct digit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def3c63",
   "metadata": {},
   "source": [
    "### 5. Visualize Hebbian network activations\n",
    "\n",
    "- Develop a way to visualize and gain a better intuition about how a Hebbian neuron's learned weights contribute to the activation across the network. One idea is to normalize each neuron's learned weights between [0, 1] (or in a way that preserves negative wts) after training. Then to visualize the contributions for a given sample `i`, scale each neuron's weights by the its netIn value to sample `i`. Draw the scaled weights using `draw_grid_image`. It might be helpful to do this for a Hebbian network with a smaller number of neurons so that you can legibly plot every neuron's weights in one plot. Analyze/interpret the relationship between this weight plot and the decoded predictions.\n",
    "- Figure out how to record the \"live\" Hebbian training weight plots into videos for later viewing.\n",
    "\n",
    "This are only two ideas — try out other ideas that come to mind!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ead03",
   "metadata": {},
   "source": [
    "### 6. Confusion matrix and error analysis of MNIST classification\n",
    "\n",
    "For one or both classifier, make a confusion matrix of the digit classifications. Use your confusion matrix to gain insight into misclassifications. Run follow-up analyses/training sessions to explore patterns in more depth. For example, if two classes are frequently misclassified, how neurons in the Hebbian network develop receptive fields that resemble each? Are the weights resembling the two classes strongly correlated (and how?)? To what degree are inhibitory weights learned for these neurons? What happens if you train the Hebbian network on only samples belonging to the two classes — do classes of either class become less/more confusable? And so forth..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6df91",
   "metadata": {},
   "source": [
    "### 7. Hebbian network in TensorFlow\n",
    "\n",
    "Implement the Hebbian network using TensorFlow rather than Numpy. Quantify/compare the runtime performance of each version.\n",
    "\n",
    "Notes: \n",
    "- Run your analysis for different mini-batch sizes. Start small, but try a wide range of sizes make sure your computer can handle the values you select. The TensorFlow version may only be faster for certain mini-batch sizes.\n",
    "- You probably will notice the biggest difference if you have GPU accelerated TensorFlow working on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492e880a",
   "metadata": {},
   "source": [
    "### 8. Implement the Generalized Hebbian Algorithm (GHA) and compare to PCA\n",
    "\n",
    "The GHA provides an incremental version of PCA — compute PCA one sample at a time over a number of training epochs. This approach can be helpful when you want to run PCA on a large dataset, but the dataset is too large to fit in your computer's memory (e.g. perhaps STL-10 at full 96x96 resolution). \n",
    "\n",
    "Implement GHA then show for a large dataset (e.g. STL-10) that GHA computes the PCA representation, whereas regular PCA (e.g. from CS251/2) fails. Plot what the image samples look like over training epochs when projected to PCA space and then back to the original data space (i.e. filtered by the learned principle components / network weights). If this sounds interesting, please see me for guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b82baa8",
   "metadata": {},
   "source": [
    "### 9. Experiment with different decoder architectures\n",
    "\n",
    "Create one or more different nonlinear decoders in TensorFlow (e.g. MLP, CNN). Compare performance/accuracy with the nonlinear one in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0397f133",
   "metadata": {},
   "source": [
    "### 10. Tuning Hebbian network\n",
    "\n",
    "Experiment with how the Hebbian network hyperparameters (number of neurons, which neuron gets inhibited, inhibition strength, etc.) affect encoding of the MNIST digits and decoding accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
