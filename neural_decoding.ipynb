{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e21d23",
   "metadata": {},
   "source": [
    "**Caleb and Florance**\n",
    "\n",
    "Spring 2023\n",
    "\n",
    "CS 443: Bio-inspired Machine Learning\n",
    "\n",
    "#### Week 1: Neural decoders\n",
    "\n",
    "# Project 1: Hebbian Learning\n",
    "\n",
    "This week you will build two different single-layer neural networks (<b>\"decoders\"</b>) for supervised learning in TensorFlow:\n",
    "1. linear softmax neural network — essentially the one in your MLP project from CS 343.\n",
    "2. nonlinear MLP implemented in Tensorflow that uses a new activation function (hyperbolic tangent).\n",
    "\n",
    "Next week, you will uses these decoding networks to see how accurately you can decode the class (int 0-9) from neural activations generated by a bio-inspired Hebbian neural network to each image in the MNIST handwritten dataset (this is like *\"mind reading\"*!). **For now, let's ignore this part** and assume that the above two neural networks that you implement will process the MNIST data directly (reshaped to the usual `(N, M)` for MLPs).\n",
    "\n",
    "**Important notes:**\n",
    "1. Your challenge is to implement `neural_decoder.py` **only with TensorFlow, no Numpy**. In other words, you should **not** import numpy at the top of `neural_decoder.py`. The reason for this is to encourage practice with the TensorFlow API.\n",
    "2. Your implementation in of both networks should only use the TensorFlow low-level API (no Keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee84588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.show()\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f47566",
   "metadata": {},
   "source": [
    "## Task 1: Linear softmax decoder\n",
    "\n",
    "The linear decoder is a single-layer softmax network. At the implementation level there is nothing different about this network than what you built last semester. This warm-up task will allow you to practice design skills that you have developed in CS 343 and apply them to a new situation. \n",
    "\n",
    "**Reminder:** This network should be built using TensorFlow only (no Numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_decoder import NeuralDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b870ad",
   "metadata": {},
   "source": [
    "### 1a. Neural decoder shared functionality: basics\n",
    "\n",
    "Start by implementing methods that both the linear and nonlinear decoders will share in the `NeuralDecoder` class (`neural_decoder.py`). Use the below test code to help you along.\n",
    "\n",
    "- Constructor\n",
    "- `get_wts(self)`\n",
    "- `get_b(self)`\n",
    "- `set_wts(self)`\n",
    "- `set_b(self)`\n",
    "- `one_hot(self, y, C, off_value=0)`\n",
    "- `accuracy(self, y_true, y_pred)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7c63f",
   "metadata": {},
   "source": [
    "#### Test: weight and bias initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "M, C = 3, 2\n",
    "nl = NeuralDecoder(M, C, wt_stdev=0.01)\n",
    "wts = nl.get_wts()\n",
    "b = nl.get_b()\n",
    "print(f\"Your wts are:\\n{wts}\\nand should be:\")\n",
    "print(\"\"\"<tf.Variable 'Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
    "array([[ 0.015,  0.004],\n",
    "       [-0.004, -0.01 ],\n",
    "       [-0.012,  0.005]], dtype=float32)>\"\"\")\n",
    "print(f\"\\nYour bias is:\\n{b}\\nand should be\")\n",
    "print(\"\"\"<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([0.011, 0.002], dtype=float32)> \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b19de",
   "metadata": {},
   "source": [
    "#### Test: one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f4c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 5\n",
    "y = tf.constant([0, 3, 2, 4])\n",
    "nl = NeuralDecoder(1, C)\n",
    "print(f'Your one-hot coding of {y} is:')\n",
    "tf.print(nl.one_hot(y, C))\n",
    "print('and should be:\\n[[1 0 0 0 0]\\n [0 0 0 1 0]\\n [0 0 1 0 0]\\n [0 0 0 0 1]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5748efb",
   "metadata": {},
   "source": [
    "#### Test: accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = NeuralDecoder(1, 1)\n",
    "y1 = tf.constant([1, 2, 3, 1, 2, 3])\n",
    "y2 = tf.constant([1, 0, 3, 1, 0, 3])\n",
    "acc = nl.accuracy(y1, y2)\n",
    "print(f'Your accuracy is {acc:.2f} and should be 0.67')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a5019e",
   "metadata": {},
   "source": [
    "### 1b. Implement Softmax decoder\n",
    "\n",
    "Create a class in `neural_decoder.py` called `SoftmaxDecoder` that inherits from `NeuralDecoder`. Implement and test the following methods in your `SoftmaxDecoder` class:\n",
    "- `forward(self, x):` Do the follow pass with samples `x`. For the softmax network, this is Dense netIn followed by softmax netAct.\n",
    "- `loss(self, yh, net_act):` Computes cross-entropy loss with true classes `yh` (one-hot coded) and `net_act`.\n",
    "\n",
    "\n",
    "For cross-entropy loss, you can use the \"one-hot version\" of the equation (*since true class values $\\vec{yh}$ are one-hot coded coming in*):\n",
    "\n",
    "$$L = -\\frac{1}{B} \\sum_{i=1}^B \\sum_{c=1}^C yh_{ic} \\times log(\\text{netAct}_{ic})$$\n",
    "\n",
    "where $B$ is the mini-batch size and $C$ is the number of classes, like usual.\n",
    "\n",
    "#### Tips\n",
    "- If you are getting type compatibility problems, try casting to `tf.float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_decoder import SoftmaxDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be77019",
   "metadata": {},
   "source": [
    "#### Test forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be73586",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, C = 4, 3, 5\n",
    "\n",
    "# test input samples\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.normal(shape=(N, M), dtype=tf.float32)\n",
    "\n",
    "sm_net = SoftmaxDecoder(M, C)\n",
    "sm_net.set_b(tf.constant([-0.062,  0.088, -0.148, 0.3, -0.002], dtype=tf.float32))\n",
    "sm_net.set_wts(tf.constant([[-0.062,  0.112,  0.127, 0.11, 0.12],\n",
    "                            [ 0.143,  0.04 ,  0.063, -0.11, -0.12],\n",
    "                            [-0.22 ,  0.189, -0.017, 0.13, 0.14]], dtype=tf.float32))\n",
    "test_net_act = sm_net.forward(x)\n",
    "\n",
    "print(f'Your net_act is:\\n{test_net_act}')\n",
    "print(f'and it should be:')\n",
    "print(\"\"\"[[0.172 0.209 0.186 0.248 0.185]\n",
    " [0.151 0.201 0.138 0.292 0.217]\n",
    " [0.181 0.237 0.17  0.237 0.174]\n",
    " [0.166 0.193 0.166 0.272 0.203]]\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e7376",
   "metadata": {},
   "source": [
    "#### Test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26237ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, C = 4, 3, 5\n",
    "\n",
    "# test input samples\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.normal(shape=(N, M), dtype=tf.float32)\n",
    "\n",
    "sm_net = SoftmaxDecoder(M, C)\n",
    "sm_net.set_b(tf.constant([-0.062,  0.088, -0.148, 0.3, -0.002], dtype=tf.float32))\n",
    "sm_net.set_wts(tf.constant([[-0.062,  0.112,  0.127, 0.11, 0.12], \\\n",
    "    [ 0.143,  0.04 ,  0.063, -0.11, -0.12], \\\n",
    "    [-0.22 ,  0.189, -0.017, 0.13, 0.14]], dtype=tf.float32))\n",
    "test_net_act = sm_net.forward(x)\n",
    "test_yh = sm_net.one_hot(y, C)\n",
    "test_loss = sm_net.loss(test_yh, test_net_act)\n",
    "\n",
    "print(f'Your loss is {test_loss:.4f} and it should be 1.5895')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730384f8",
   "metadata": {},
   "source": [
    "### 1c. Neural decoder shared functionality (`predict`)\n",
    "\n",
    "Now that you've implemented the specific methods for the softmax decoder, implement the `predict(self, x, net_act=None)` in the `NeuralDecoder` class, which predicts the class of each sample in `x`. This method is common to both the linear and nonlinear decoders (`neural_decoder.py`):\n",
    "\n",
    "To predict the class of sample `i` $c_i^*$, you can use softmax:\n",
    "$$c_i^* = argmax_{c}(\\text{netAct}_{ic})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e9bfea",
   "metadata": {},
   "source": [
    "#### Test `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, C = 4, 3, 3\n",
    "\n",
    "# test input samples\n",
    "tf.random.set_seed(5)\n",
    "x = tf.random.normal(shape=(N, M), dtype=tf.float32)\n",
    "\n",
    "sm_net = SoftmaxDecoder(M, C)\n",
    "sm_net.set_b(tf.constant([-0.062,  0.088, -0.148], dtype=tf.float32))\n",
    "sm_net.set_wts(tf.constant([[-0.062,  0.112,  0.127], \\\n",
    "    [ 0.143,  0.04 ,  0.063], \\\n",
    "    [-0.22 ,  0.189, -0.017]], dtype=tf.float32))\n",
    "test_preds = sm_net.predict(x)\n",
    "\n",
    "tf.print(f'Your test predicted classes are {test_preds} and they should be [1 0 1 1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1d53a",
   "metadata": {},
   "source": [
    "### 1d. Neural decoder shared functionality (`fit` with early stopping)\n",
    "\n",
    "To make determining a reasonable number of training epochs less of a chore, implement training with **early stopping**: train and regularly monitor loss on the validation set. If the network has not \"broken a new record\" in making the validation loss smaller over the recent history, stop training the network. We also limit training to some maximum number of epochs in case we do not stop early to prevent the network possibly training for an unreasonably long time. The number of validation loss values in the recent history that we consider when deciding whether to stop training early is called the **patience**.\n",
    "\n",
    "#### Todo\n",
    "\n",
    "Implement the following training related methods in the `NeuralDecoder` class (`neural_decoder.py`). These methods are shared among all types of decoder networks (linear, nonlinear).\n",
    "\n",
    "- `early_stopping(self, recent_val_losses, curr_val_loss, patience)`: Update rolling list of recent validation loss values with the validation loss computed on the current epoch. Return whether we should stop training based on the `patience` hyperparameter value.\n",
    "- `extract_at_indices(self, x, indices)`: small helper method to return the samples or labels (`x`) at the indices `indices`. Useful for forming mini-batches during training.\n",
    "- `fit(self, x, y, x_val=None, y_val=None, mini_batch_sz=512, lr=1e-4, max_epochs=1000, patience=3, val_freq=1, verbose=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f524cf6",
   "metadata": {},
   "source": [
    "#### Test `early_stopping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607650e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SoftmaxDecoder(1, 1)\n",
    "\n",
    "# Test 1\n",
    "patience_1 = 5\n",
    "es_lost_hist_1 = []\n",
    "for iter in range(10):\n",
    "    curr_loss = float(iter)\n",
    "    es_lost_hist_1, stop = sd.early_stopping(es_lost_hist_1, curr_loss, patience=patience_1)\n",
    "\n",
    "    if stop:\n",
    "        break\n",
    "print(f'Early stopping Test 1 ({patience_1=}):\\n Stopped after {iter} iterations (should be 5 iterations).')\n",
    "print(f' Recent loss history is {es_lost_hist_1} and should be [1.0, 2.0, 3.0, 4.0, 5.0]')\n",
    "print()\n",
    "\n",
    "# Test 2\n",
    "tf.random.set_seed(1)\n",
    "patience_2 = 3\n",
    "es_lost_hist_2 = []\n",
    "test_2_loss_vals = list(tf.random.uniform(shape=(20,)).numpy())\n",
    "for iter in range(30):\n",
    "    curr_loss = test_2_loss_vals[iter]\n",
    "    es_lost_hist_2, stop = sd.early_stopping(es_lost_hist_2, curr_loss, patience=patience_2)\n",
    "\n",
    "    if stop:\n",
    "        break\n",
    "print(f'Early stopping Test 2 ({patience_2=}):\\n Stopped after {iter} iterations (should be 6 iterations).')\n",
    "print(f' Recent loss history is {es_lost_hist_2} and should be [0.29193902, 0.64250207, 0.9757855]')\n",
    "print()\n",
    "\n",
    "# Test 3\n",
    "tf.random.set_seed(1)\n",
    "patience_3 = 6\n",
    "es_lost_hist_3 = []\n",
    "test_3_loss_vals = list(tf.random.uniform(shape=(20,)).numpy())\n",
    "for iter in range(30):\n",
    "    curr_loss = test_3_loss_vals[iter]\n",
    "    es_lost_hist_3, stop = sd.early_stopping(es_lost_hist_3, curr_loss, patience=patience_3)\n",
    "\n",
    "    if stop:\n",
    "        break\n",
    "print(f'Early stopping Test 3 ({patience_3=}):\\n Stopped after {iter} iterations (should be 9 iterations).')\n",
    "print(f' Recent loss history is\\n {es_lost_hist_3}\\n and should be')\n",
    "print(f' [0.29193902, 0.64250207, 0.9757855, 0.43509948, 0.6601019, 0.60489583]')\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa498b",
   "metadata": {},
   "source": [
    "#### Test `extract_at_indices`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SoftmaxDecoder(1, 1)\n",
    "\n",
    "# Test 1\n",
    "test_samples = tf.reshape(tf.range(100, dtype=tf.float32), [25, 4])\n",
    "test_1_inds = tf.constant([0, 10, 2])\n",
    "test_1_vals = sd.extract_at_indices(test_samples, test_1_inds)\n",
    "print(f'Test 1:\\n-------\\n {test_1_vals=}\\n and should be')\n",
    "print(\"\"\" test_1_vals=<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
    "array([[ 0.,  1.,  2.,  3.],\n",
    "       [40., 41., 42., 43.],\n",
    "       [ 8.,  9., 10., 11.]], dtype=float32)>\"\"\")\n",
    "\n",
    "# Test 2\n",
    "tf.random.set_seed(0)\n",
    "test_labels = tf.random.shuffle(tf.range(10))\n",
    "test_2_inds = tf.constant([1, 2, 3, 4])\n",
    "test_2_vals = sd.extract_at_indices(test_labels, test_2_inds)\n",
    "print(f'Test 2:\\n-------\\n {test_2_vals=}\\n and should be')\n",
    "print(\"\"\" test_2_vals=<tf.Tensor: shape=(4,), dtype=int32, numpy=array([9, 1, 2, 7], dtype=int32)>\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e09955b",
   "metadata": {},
   "source": [
    "#### Test `fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f50d6",
   "metadata": {},
   "source": [
    "Write code in the cell below to:\n",
    "- Load in Iris training and validation sets (as you have done above for your softmax classifier). For this test, **don't** do any preprocessing (e.g. no normalization).\n",
    "- Make sure that your train and validation samples are `tf.float32`.\n",
    "- Create and train a softmax decoder network using the hyperparameters provided below.\n",
    "- Make a well-labeled plot showing the validation loss over training epochs.\n",
    "\n",
    "If everything is working as expected, your validation accuracy should reach ~98% by the end of training and the validation loss should reach ~0.08. Everyone's epochs until early stopping might be different. Mine stopped after 700 epochs.\n",
    "\n",
    "You may use Numpy as needed when writing code in this notebook.\n",
    "\n",
    "Suggested information to include in training progress print outs (**note:** the numbers below are fake placeholders):\n",
    "\n",
    "```\n",
    "Epoch 0/4999, Training loss 10.00, Val loss 5.00, Val acc 10.00\n",
    "Epoch 100/4999, Training loss 9.00, Val loss 4.00, Val acc 15.00\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c267f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # keep me\n",
    "\n",
    "# Use these parameters in your softmax net\n",
    "mini_batch_sz = 25\n",
    "lr = 1e-1\n",
    "max_epochs = 5000\n",
    "patience = 3\n",
    "val_every = 100  # how often (in epochs) we check the val loss/acc/early stopping\n",
    "\n",
    "# TODO: Your code here:\n",
    "x = np.load('iris_train_samps.npy')\n",
    "y = np.load('iris_train_labels.npy')\n",
    "print(f'{x.shape = }')\n",
    "print(f'{y.shape = }')\n",
    "\n",
    "x_val = np.load('iris_val_samps.npy')\n",
    "y_val = np.load('iris_val_labels.npy')\n",
    "print(f'{x_val.shape = }')\n",
    "print(f'{y_val.shape = }')\n",
    "sd = SoftmaxDecoder(4, 3)\n",
    "\n",
    "sd.fit(x, y, x_val, y_val, mini_batch_sz=mini_batch_sz, lr=lr, max_epochs=max_epochs, patience=patience, val_every=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d76177",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred = sd.predict(x_val)\n",
    "(pred == y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20c410",
   "metadata": {},
   "source": [
    "### 1e. Questions: Effect of early stopping\n",
    "\n",
    "**Question 1:** Copy-and-paste your training code from the cell above. Change the `patience` hyperparameter. How do increases and decreases affect the duration of training and **why**?\n",
    "\n",
    "**Question 2:** Explain how the choice of `patience` affects the risk of overfitting.\n",
    "\n",
    "**Question 3:** For Iris, does it seem easy or hard to overfit the training set? **What makes you think so?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1a9b1",
   "metadata": {},
   "source": [
    "**Answer 1:**\n",
    "\n",
    "**Answer 2:**\n",
    "\n",
    "**Answer 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c164a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3720dfa",
   "metadata": {},
   "source": [
    "## Task 2: Nonlinear decoder\n",
    "\n",
    "Now that you have the softmax decoder implemented, implement in TensorFlow the following nonlinear single layer decoding neural network proposed by Krotov & Hopfield (2019):\n",
    "\n",
    "$$\\text{netIn}_{ic} = \\sum_{j=1}^M x_{ij}w_{jc} + b_c$$\n",
    "$$\\text{netAct}_{ic} = tanh(\\beta * \\text{netIn}_{ic})$$\n",
    "\n",
    "where:\n",
    "- the `net_act` function is the hyperbolic tangent function `tanh` (*this is another common net_act function in neural networks*).\n",
    "- the wts have shape `(M, C)` and the bias has shape `(C,)`.\n",
    "- the input $x_{ij}$ has the usual shape `(N, M)`.\n",
    "\n",
    "To predict the class of sample `i` $c_i^*$, use softmax (same as in your softmax network):\n",
    "$$c_i^* = argmax_{c}(\\text{netAct}_{ic})$$\n",
    "\n",
    "The loss function to minimize is:\n",
    "\n",
    "$$L = \\sum_{i=1}^B \\sum_{c=1}^C \\left | \\text{yh}_{ic} - \\text{netAct}_{ic} \\right |^m$$\n",
    "\n",
    "where:\n",
    "- $\\text{yh}_{ic}$ is the one-hot coding of the true class of sample `i`. Remember: because we are using the $tanh$ activation function, the \"on\" value is `1` (like usual), **but the \"off\" value is `-1`**.\n",
    "- The exponent `m` is a hyperparameter. By default we assume `m=6`.\n",
    "- $|\\cdot|$ means absolute value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6768adec",
   "metadata": {},
   "source": [
    "### 2a. Implement nonlinear decoder class\n",
    "\n",
    "Make a class `NonlinearDecoder` in `neural_decoder.py` that inherits from `NeuralDecoder`. Implement the following methods in the `NonlinearDecoder` class.\n",
    "- `__init__(self, num_features, num_classes, wt_stdev=0.1, beta=0.005, loss_exp=6):` Set instance variables for `beta` ($\\beta$) and `loss_exp` ($m$). Extend parent class constructor/exploit inheritance. \n",
    "- `one_hot(self, y, C)`\n",
    "- `forward(self, x)`\n",
    "- `loss(self, yh, net_act)`\n",
    "\n",
    "All of these methods should require only a few lines of code each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d07ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_decoder import NonlinearDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd7d78",
   "metadata": {},
   "source": [
    "#### Test: weight and bias initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef096d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "M, C = 4, 3\n",
    "nl = NonlinearDecoder(M, C)\n",
    "wts = nl.get_wts()\n",
    "b = nl.get_b()\n",
    "\n",
    "tf.print(f\"Your wts are:\\n{wts}\\nand it should be:\")\n",
    "print(\"\"\"<tf.Variable 'Variable:0' shape=(4, 3) dtype=float32, numpy=\n",
    "array([[-0.11 ,  0.155,  0.038],\n",
    "       [-0.088, -0.122, -0.098],\n",
    "       [ 0.009, -0.02 , -0.056],\n",
    "       [-0.072, -0.063, -0.072]], dtype=float32)>\"\"\")\n",
    "tf.print(f\"\\nYour bias is:\\n{b}\\nand it should be\")\n",
    "print(f\"<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([ 0.04 , -0.109, -0.006], dtype=float32)>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d117f58f",
   "metadata": {},
   "source": [
    "#### Test forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f87baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, C = 4, 3, 3\n",
    "\n",
    "# test input samples\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.normal(shape=(N, M), dtype=tf.float32) + 100\n",
    "\n",
    "nl_net = NonlinearDecoder(M, C)\n",
    "nl_net.set_b(tf.constant([-0.062,  0.088, -0.148], dtype=tf.float32))\n",
    "nl_net.set_wts(tf.constant([[-0.062,  0.112,  0.127],\n",
    "                            [ 0.143,  0.04 ,  0.063],\n",
    "                            [-0.22 ,  0.189, -0.017]], dtype=tf.float32))\n",
    "test_net_act = nl_net.forward(x, net_act=True)\n",
    "\n",
    "print(f'Your net_act is:\\n{test_net_act}')\n",
    "print(f'and it should be:')\n",
    "print(\"\"\"[[-0.069  0.17   0.087]\n",
    " [-0.071  0.169  0.084]\n",
    " [-0.07   0.17   0.086]\n",
    " [-0.07   0.169  0.086]]\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684a1b1",
   "metadata": {},
   "source": [
    "#### Test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, C = 4, 3, 3\n",
    "\n",
    "# test input samples\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.normal(shape=(N, M), dtype=tf.float32)\n",
    "y = np.array([2, 0, 1, 1])\n",
    "\n",
    "sm_net = NonlinearDecoder(M, C)\n",
    "sm_net.set_b(tf.constant([-0.062,  0.088, -0.148], dtype=tf.float32))\n",
    "sm_net.set_wts(tf.constant([[-0.062,  0.112,  0.127],\n",
    "                            [ 0.143,  0.04 ,  0.063],\n",
    "                            [-0.22 ,  0.189, -0.017]], dtype=tf.float32))\n",
    "test_yh = sm_net.one_hot(y, C)\n",
    "test_net_act = sm_net.forward(x, net_act=True)\n",
    "test_loss = sm_net.loss(test_yh, test_net_act)\n",
    "\n",
    "print(f'Your loss is {test_loss:.4f} and it should be 11.9828')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f689c",
   "metadata": {},
   "source": [
    "#### Test `fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc6027",
   "metadata": {},
   "source": [
    "Copy-and-paste your code from above that trains the softmax network on Iris below. Modify the code in the cell below to train your Nonlinear decoder instead and make the same plot of the validation loss.\n",
    "\n",
    "Use the same training hyperparameters as you used with the softmax network and default values for the β and `m` (loss exponent) hyperparameters. \n",
    "\n",
    "If everything is working as expected, your validation accuracy should reach ~98% by the end of training and the validation loss should reach ~42. Everyone's early stopping values might be different. In case it helps to know, mine stopped after 3400 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5986d4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31165aff",
   "metadata": {},
   "source": [
    "### 2b. Effect of β and loss exponent\n",
    "\n",
    "Make high-quality, well-labeled plots showing:\n",
    "1. Validation loss over epoch when the nonlinear decoder loss exponent hyperparameter is: 1, 3, 6, 9. There should therefore be 4 curves plotted in the same plot (labeled with legend). Keep β at its default value.\n",
    "2. Validation loss over epoch when the nonlinear decoder hyperparameter β is: 0.001, 0.005, 0.01. There should therefore be 3 curves plotted in the same plot (labeled with legend). Keep the loss exponent at its default value.\n",
    "\n",
    "In both cases, set the remaining hyperparameters to the values used in Task 2a above except:\n",
    "- Set `patience` to 2 and lower `max_epochs` (e.g. to 2000) to speed things up. For this test, it is ok if the network does not stop early (i.e. trains for the full `max_epochs` and then stops).\n",
    "- Turn off `verbose` so that you do not get any print outs as each decoder is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these parameters\n",
    "mini_batch_sz = 25\n",
    "lr = 1e-1\n",
    "max_epochs = 2000\n",
    "patience = 2\n",
    "val_freq = 100  # how often (in epochs) we check the val loss/acc/early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d62f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of loss exponent m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da889a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5dbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22c922",
   "metadata": {},
   "source": [
    "### 2c. Questions\n",
    "\n",
    "**Question 4:** Explain what you observe about the effect of the loss exponent `m` on the validation loss. What does it indicate about the quality and speed of learning?\n",
    "\n",
    "**Question 5:** Explain what you observe about the effect of β on the validation loss. What does it indicate about the quality and speed of learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ade66",
   "metadata": {},
   "source": [
    "**Answer 4:**\n",
    "\n",
    "**Answer 5:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "980e4738edd8f3d052d9d2e0f8a87660df97b89186b811d0c0693c4fc3296814"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
